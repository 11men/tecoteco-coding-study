# í¬ë¡¤ë§ ì‹œìŠ¤í…œ êµ¬í˜„ ê°€ì´ë“œ

## ëª©ì°¨

1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)
2. [í”„ë¡œì íŠ¸ êµ¬ì¡°](#2-í”„ë¡œì íŠ¸-êµ¬ì¡°)
3. [Step-by-Step êµ¬í˜„](#3-step-by-step-êµ¬í˜„)
4. [í…ŒìŠ¤íŠ¸](#4-í…ŒìŠ¤íŠ¸)
5. [ë°°í¬](#5-ë°°í¬)
6. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#6-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## 1. í™˜ê²½ ì„¤ì •

### 1.1 í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

```bash
# Node.js 18+ ì„¤ì¹˜ í™•ì¸
node --version  # v18.0.0 ì´ìƒ

# PostgreSQL ì„¤ì¹˜ (Docker ê¶Œì¥)
docker run -d \
  --name postgres \
  -e POSTGRES_USER=busstrike \
  -e POSTGRES_PASSWORD=password \
  -e POSTGRES_DB=busstrike \
  -p 5432:5432 \
  postgres:15

# Redis ì„¤ì¹˜ (Docker)
docker run -d \
  --name redis \
  -p 6379:6379 \
  redis:7-alpine
```

### 1.2 í”„ë¡œì íŠ¸ ì´ˆê¸°í™”

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir busstrike-crawler
cd busstrike-crawler

# package.json ì´ˆê¸°í™”
npm init -y

# ì˜ì¡´ì„± ì„¤ì¹˜
npm install \
  playwright \
  cheerio \
  axios \
  node-cron \
  pg \
  redis \
  winston \
  dotenv

# ê°œë°œ ì˜ì¡´ì„±
npm install -D \
  @types/node \
  typescript \
  ts-node \
  @types/jest \
  jest \
  nodemon
```

### 1.3 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
# Database
DATABASE_URL=postgresql://busstrike:password@localhost:5432/busstrike

# Redis
REDIS_URL=redis://localhost:6379

# Crawler Settings
CRAWLER_INTERVAL_TIER1=30  # minutes (realistic interval)
CRAWLER_INTERVAL_TIER2=120 # 2 hours for news

# Playwright
PLAYWRIGHT_HEADLESS=true

# Logging
LOG_LEVEL=info

# Notification (ë‚˜ì¤‘ì— êµ¬í˜„)
FCM_SERVER_KEY=
SLACK_WEBHOOK_URL=
EOF
```

### 1.4 TypeScript ì„¤ì •

```bash
# tsconfig.json ìƒì„±
cat > tsconfig.json << EOF
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "lib": ["ES2020"],
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
EOF
```

---

## 2. í”„ë¡œì íŠ¸ êµ¬ì¡°

```
busstrike-crawler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawlers/           # í¬ë¡¤ëŸ¬ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ base.ts
â”‚   â”‚   â”œâ”€â”€ topis.ts
â”‚   â”‚   â”œâ”€â”€ gbis.ts
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ services/           # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ strike.ts       # íŒŒì—… ì •ë³´ ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ notification.ts # ì•Œë¦¼ ë°œì†¡
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ models/             # ë°ì´í„° ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ database.ts     # DB ì—°ê²°
â”‚   â”‚   â”œâ”€â”€ crawl-source.ts
â”‚   â”‚   â”œâ”€â”€ raw-notice.ts
â”‚   â”‚   â””â”€â”€ strike-event.ts
â”‚   â”œâ”€â”€ utils/              # ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ logger.ts
â”‚   â”‚   â”œâ”€â”€ redis.ts
â”‚   â”‚   â””â”€â”€ hash.ts
â”‚   â”œâ”€â”€ scheduler.ts        # í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”œâ”€â”€ app.ts              # Admin API
â”‚   â””â”€â”€ index.ts            # ì§„ì…ì 
â”œâ”€â”€ tests/                  # í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ crawlers/
â”‚   â””â”€â”€ services/
â”œâ”€â”€ scripts/                # ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ init-db.sql
â”‚   â””â”€â”€ seed-sources.sql
â”œâ”€â”€ .env
â”œâ”€â”€ tsconfig.json
â””â”€â”€ package.json
```

---

## 3. Step-by-Step êµ¬í˜„

### Step 1: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”

```bash
# scripts/init-db.sql
cat > scripts/init-db.sql << 'EOF'
-- í…Œì´ë¸” ìƒì„±
CREATE TABLE crawl_sources (
    id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    url TEXT NOT NULL,
    type VARCHAR(20) NOT NULL CHECK (type IN ('official', 'news', 'sns')),
    priority INT NOT NULL CHECK (priority BETWEEN 1 AND 3),
    interval_minutes INT NOT NULL DEFAULT 10,
    is_active BOOLEAN NOT NULL DEFAULT true,
    last_crawled_at TIMESTAMP,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE TABLE crawl_logs (
    id VARCHAR(50) PRIMARY KEY,
    source_id VARCHAR(50) NOT NULL REFERENCES crawl_sources(id),
    status VARCHAR(20) NOT NULL CHECK (status IN ('success', 'failed', 'partial')),
    items_found INT NOT NULL DEFAULT 0,
    items_processed INT NOT NULL DEFAULT 0,
    error_message TEXT,
    duration_ms INT,
    crawled_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE TABLE raw_notices (
    id VARCHAR(50) PRIMARY KEY,
    source_id VARCHAR(50) NOT NULL REFERENCES crawl_sources(id),
    crawl_log_id VARCHAR(50) REFERENCES crawl_logs(id),
    title TEXT NOT NULL,
    content TEXT,
    url TEXT,
    category VARCHAR(50),
    published_at TIMESTAMP,
    views INT,
    has_attachment BOOLEAN DEFAULT false,
    content_hash VARCHAR(32) NOT NULL,
    metadata JSONB,
    crawled_at TIMESTAMP NOT NULL DEFAULT NOW(),
    UNIQUE (source_id, content_hash)
);

CREATE TABLE strike_events (
    id VARCHAR(50) PRIMARY KEY,
    raw_notice_id VARCHAR(50) REFERENCES raw_notices(id),
    title VARCHAR(200) NOT NULL,
    description TEXT,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    start_date TIMESTAMP,
    end_date TIMESTAMP,
    affected_regions TEXT[] DEFAULT '{}',
    affected_companies TEXT[] DEFAULT '{}',
    affected_route_count INT DEFAULT 0,
    source VARCHAR(50) NOT NULL,
    source_url TEXT,
    confidence_score DECIMAL(3,2),
    detected_at TIMESTAMP NOT NULL DEFAULT NOW(),
    confirmed_at TIMESTAMP
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_crawl_logs_source ON crawl_logs(source_id);
CREATE INDEX idx_raw_notices_hash ON raw_notices(content_hash);
CREATE INDEX idx_strike_events_status ON strike_events(status);
EOF

# DB ì´ˆê¸°í™” ì‹¤í–‰
docker exec -i postgres psql -U busstrike -d busstrike < scripts/init-db.sql

# ì´ˆê¸° ë°ì´í„° ì…ë ¥
cat > scripts/seed-sources.sql << 'EOF'
INSERT INTO crawl_sources (id, name, url, type, priority, interval_minutes) VALUES
('topis', 'TOPIS', 'https://topis.seoul.go.kr/notice/openNoticeList.do', 'official', 1, 10),
('gbis', 'GBIS', 'https://www.gbis.go.kr/gbis2014/bbs.action?cmd=notice', 'official', 1, 10),
('ictr', 'ICTR', 'https://www.ictr.or.kr/board/notice.do', 'official', 1, 30);
EOF

docker exec -i postgres psql -U busstrike -d busstrike < scripts/seed-sources.sql
```

### Step 2: ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° êµ¬í˜„

```typescript
// src/utils/logger.ts
import winston from 'winston';

export const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.File({ filename: 'logs/error.log', level: 'error' }),
    new winston.transports.File({ filename: 'logs/crawler.log' }),
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});
```

```typescript
// src/utils/redis.ts
import { createClient } from 'redis';
import { logger } from './logger';

class RedisClient {
  private client;

  constructor() {
    this.client = createClient({
      url: process.env.REDIS_URL
    });

    this.client.on('error', (err) => logger.error('Redis error:', err));
  }

  async connect() {
    await this.client.connect();
    logger.info('Redis connected');
  }

  async get(key: string): Promise<string | null> {
    return await this.client.get(key);
  }

  async set(key: string, value: string, ttl?: number): Promise<void> {
    if (ttl) {
      await this.client.setEx(key, ttl, value);
    } else {
      await this.client.set(key, value);
    }
  }

  async del(key: string): Promise<void> {
    await this.client.del(key);
  }

  async setNX(key: string, value: string, ttl: number): Promise<boolean> {
    const result = await this.client.set(key, value, {
      NX: true,
      EX: ttl
    });
    return result === 'OK';
  }
}

export const redis = new RedisClient();
```

```typescript
// src/utils/hash.ts
import crypto from 'crypto';

export function generateHash(text: string): string {
  return crypto.createHash('md5').update(text).digest('hex');
}

export function generateId(prefix: string): string {
  return `${prefix}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
}
```

### Step 3: ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸

```typescript
// src/models/database.ts
import { Pool } from 'pg';
import { logger } from '../utils/logger';

class Database {
  private pool: Pool;

  constructor() {
    this.pool = new Pool({
      connectionString: process.env.DATABASE_URL,
      max: 20,
      idleTimeoutMillis: 30000,
      connectionTimeoutMillis: 2000,
    });

    this.pool.on('error', (err) => {
      logger.error('Unexpected database error:', err);
    });
  }

  async query(text: string, params?: any[]) {
    const start = Date.now();
    try {
      const result = await this.pool.query(text, params);
      const duration = Date.now() - start;
      logger.debug('Executed query', { text, duration, rows: result.rowCount });
      return result;
    } catch (error) {
      logger.error('Database query error', { text, error });
      throw error;
    }
  }

  async transaction(callback: (client: any) => Promise<void>) {
    const client = await this.pool.connect();
    try {
      await client.query('BEGIN');
      await callback(client);
      await client.query('COMMIT');
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }
}

export const db = new Database();
```

```typescript
// src/models/raw-notice.ts
import { db } from './database';
import { generateId, generateHash } from '../utils/hash';

export interface RawNotice {
  id: string;
  sourceId: string;
  crawlLogId?: string;
  title: string;
  content?: string;
  url?: string;
  category?: string;
  publishedAt?: Date;
  views?: number;
  hasAttachment?: boolean;
  contentHash: string;
  metadata?: any;
  crawledAt: Date;
}

export class RawNoticeModel {
  async create(notice: Omit<RawNotice, 'id' | 'crawledAt'>): Promise<RawNotice> {
    const id = generateId('notice');
    const result = await db.query(
      `INSERT INTO raw_notices
       (id, source_id, crawl_log_id, title, content, url, category,
        published_at, views, has_attachment, content_hash, metadata)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
       RETURNING *`,
      [
        id, notice.sourceId, notice.crawlLogId, notice.title, notice.content,
        notice.url, notice.category, notice.publishedAt, notice.views,
        notice.hasAttachment, notice.contentHash, JSON.stringify(notice.metadata)
      ]
    );
    return result.rows[0];
  }

  async findByHash(sourceId: string, hash: string): Promise<RawNotice | null> {
    const result = await db.query(
      'SELECT * FROM raw_notices WHERE source_id = $1 AND content_hash = $2',
      [sourceId, hash]
    );
    return result.rows[0] || null;
  }

  async findByKeyword(keyword: string, limit = 20): Promise<RawNotice[]> {
    const result = await db.query(
      `SELECT * FROM raw_notices
       WHERE title ILIKE $1 OR content ILIKE $1
       ORDER BY published_at DESC
       LIMIT $2`,
      [`%${keyword}%`, limit]
    );
    return result.rows;
  }
}

export const rawNoticeModel = new RawNoticeModel();
```

### Step 4: í¬ë¡¤ëŸ¬ êµ¬í˜„

```typescript
// src/crawlers/base.ts
import { chromium, Browser, Page } from 'playwright';
import { logger } from '../utils/logger';

export interface CrawlResult {
  sourceId: string;
  items: any[];
  duration: number;
  error?: string;
}

export abstract class BaseCrawler {
  protected sourceId: string;
  protected sourceName: string;
  protected baseUrl: string;
  protected browser: Browser | null = null;

  constructor(sourceId: string, sourceName: string, baseUrl: string) {
    this.sourceId = sourceId;
    this.sourceName = sourceName;
    this.baseUrl = baseUrl;
  }

  async initialize(): Promise<void> {
    this.browser = await chromium.launch({
      headless: process.env.PLAYWRIGHT_HEADLESS !== 'false'
    });
    logger.info(`${this.sourceName} crawler initialized`);
  }

  async cleanup(): Promise<void> {
    if (this.browser) {
      await this.browser.close();
      this.browser = null;
    }
  }

  abstract crawl(): Promise<CrawlResult>;

  protected async createPage(): Promise<Page> {
    if (!this.browser) {
      throw new Error('Browser not initialized');
    }
    return await this.browser.newPage();
  }
}
```

```typescript
// src/crawlers/topis.ts
import { BaseCrawler, CrawlResult } from './base';
import { logger } from '../utils/logger';

export class TOPISCrawler extends BaseCrawler {
  constructor() {
    super('topis', 'TOPIS', 'https://topis.seoul.go.kr/notice/openNoticeList.do');
  }

  async crawl(): Promise<CrawlResult> {
    const startTime = Date.now();
    const items: any[] = [];

    try {
      const page = await this.createPage();
      await page.goto(this.baseUrl, { waitUntil: 'networkidle', timeout: 30000 });

      // ë²„ìŠ¤ì•ˆë‚´ íƒ­ í´ë¦­
      const busTab = page.locator('a:has-text("ë²„ìŠ¤ì•ˆë‚´")');
      if (await busTab.count() > 0) {
        await busTab.click();
        await page.waitForTimeout(2000);
      }

      // ê³µì§€ì‚¬í•­ ëª©ë¡ ì¶”ì¶œ
      const notices = await page.$$eval('table tbody tr', (rows) => {
        return rows.map(row => {
          const cells = row.querySelectorAll('td');
          return {
            number: cells[0]?.textContent?.trim() || '',
            title: cells[1]?.querySelector('a')?.textContent?.trim() || '',
            link: cells[1]?.querySelector('a')?.getAttribute('href') || '',
            hasAttachment: !!cells[2]?.querySelector('img'),
            date: cells[3]?.textContent?.trim() || '',
            views: parseInt(cells[4]?.textContent?.trim() || '0', 10)
          };
        });
      });

      // íŒŒì—… ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
      const strikeKeywords = ['íŒŒì—…', 'ìš´í–‰ì¤‘ë‹¨', 'ë…¸ì‚¬í˜‘ìƒ', 'ë‹¨ì²´êµì„­'];
      const filteredNotices = notices.filter(notice =>
        strikeKeywords.some(keyword => notice.title.includes(keyword))
      );

      logger.info(`TOPIS: Found ${notices.length} notices, ${filteredNotices.length} strike-related`);

      // ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (í•„í„°ë§ëœ ê²ƒë§Œ)
      for (const notice of filteredNotices) {
        try {
          const detailLink = page.locator(`a:has-text("${notice.title}")`).first();
          await detailLink.click();
          await page.waitForTimeout(1000);

          const content = await page.$eval('.content', el => el.textContent || '');

          items.push({
            ...notice,
            content: content.trim(),
            sourceId: this.sourceId,
            sourceName: this.sourceName
          });

          await page.goBack();
          await page.waitForTimeout(500);
        } catch (error) {
          logger.error(`Failed to get detail for: ${notice.title}`, error);
        }
      }

      await page.close();

      return {
        sourceId: this.sourceId,
        items,
        duration: Date.now() - startTime
      };

    } catch (error: any) {
      logger.error(`TOPIS crawl failed:`, error);
      return {
        sourceId: this.sourceId,
        items,
        duration: Date.now() - startTime,
        error: error.message
      };
    }
  }
}
```

```typescript
// src/crawlers/gbis.ts
import { BaseCrawler, CrawlResult } from './base';
import { logger } from '../utils/logger';

export class GBISCrawler extends BaseCrawler {
  constructor() {
    super('gbis', 'GBIS', 'https://www.gbis.go.kr/gbis2014/bbs.action?cmd=notice');
  }

  async crawl(): Promise<CrawlResult> {
    const startTime = Date.now();
    const items: any[] = [];

    try {
      const page = await this.createPage();
      await page.goto(this.baseUrl, { waitUntil: 'networkidle', timeout: 30000 });

      // ì‹œë‚´ë²„ìŠ¤ê³µì§€ í•„í„° ì„ íƒ (ìˆëŠ” ê²½ìš°)
      try {
        const categorySelect = page.locator('select[name="category"]');
        if (await categorySelect.count() > 0) {
          await categorySelect.selectOption('ì‹œë‚´ë²„ìŠ¤ê³µì§€');
          await page.click('button[type="submit"]');
          await page.waitForTimeout(2000);
        }
      } catch (error) {
        logger.warn('GBIS: Category filter not found, continuing...');
      }

      // ê³µì§€ì‚¬í•­ ëª©ë¡ ì¶”ì¶œ
      const notices = await page.$$eval('table tbody tr', (rows) => {
        return rows.map(row => {
          const cells = row.querySelectorAll('td');
          return {
            number: cells[0]?.textContent?.trim() || '',
            title: cells[1]?.textContent?.trim() || '',
            category: cells[2]?.textContent?.trim() || '',
            date: cells[3]?.textContent?.trim() || '',
            views: parseInt(cells[4]?.textContent?.trim() || '0', 10)
          };
        });
      });

      // íŒŒì—… ê´€ë ¨ í•„í„°ë§
      const strikeKeywords = ['íŒŒì—…', 'ìš´í–‰ì¤‘ë‹¨', 'ë²„ìŠ¤ìš´í–‰'];
      const filteredNotices = notices.filter(notice =>
        strikeKeywords.some(keyword => notice.title.includes(keyword))
      );

      logger.info(`GBIS: Found ${notices.length} notices, ${filteredNotices.length} strike-related`);

      items.push(...filteredNotices.map(notice => ({
        ...notice,
        sourceId: this.sourceId,
        sourceName: this.sourceName
      })));

      await page.close();

      return {
        sourceId: this.sourceId,
        items,
        duration: Date.now() - startTime
      };

    } catch (error: any) {
      logger.error(`GBIS crawl failed:`, error);
      return {
        sourceId: this.sourceId,
        items,
        duration: Date.now() - startTime,
        error: error.message
      };
    }
  }
}
```

### Step 5: íŒŒì—… ì •ë³´ ì²˜ë¦¬ ì„œë¹„ìŠ¤

```typescript
// src/services/strike.ts
import { rawNoticeModel } from '../models/raw-notice';
import { db } from '../models/database';
import { generateId, generateHash } from '../utils/hash';
import { redis } from '../utils/redis';
import { logger } from '../utils/logger';

export class StrikeService {
  async processNotice(crawlLogId: string, sourceId: string, notice: any): Promise<void> {
    const contentHash = generateHash(notice.title + notice.date);

    // Redis ì¤‘ë³µ ì²´í¬ (ì›ë³¸ ê³µì§€ ì¤‘ë³µë§Œ ì²´í¬)
    const cacheKey = `notice:${sourceId}:${contentHash}`;
    const exists = await redis.get(cacheKey);

    if (exists) {
      logger.debug(`Notice already processed: ${notice.title}`);
      return;
    }

    // DBì— ì›ë³¸ ê³µì§€ ì €ì¥
    const rawNotice = await rawNoticeModel.create({
      sourceId,
      crawlLogId,
      title: notice.title,
      content: notice.content,
      url: notice.link || notice.url,
      category: notice.category,
      publishedAt: this.parseDate(notice.date),
      views: notice.views,
      hasAttachment: notice.hasAttachment || false,
      contentHash,
      metadata: notice
    });

    // Redis ìºì‹œ (24ì‹œê°„)
    await redis.set(cacheKey, '1', 86400);

    // íŒŒì—… ì •ë³´ ì¶”ì¶œ
    const strikeInfo = this.extractStrikeInfo(notice);

    if (strikeInfo.isStrike) {
      // ê¸°ì¡´ íŒŒì—… ì´ë²¤íŠ¸ ì°¾ê¸° (ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜)
      const existingStrike = await this.findSimilarStrikeEvent(strikeInfo);

      if (existingStrike) {
        // ê¸°ì¡´ íŒŒì—… ì—…ë°ì´íŠ¸ ë° ë³€ê²½ì‚¬í•­ ê°ì§€
        await this.updateStrikeEventIfChanged(existingStrike, strikeInfo, rawNotice.id, sourceId);
      } else {
        // ì‹ ê·œ íŒŒì—… ì´ë²¤íŠ¸ ìƒì„± ë° ì•Œë¦¼
        await this.createStrikeEvent(rawNotice.id, strikeInfo, sourceId, true);
      }
    }
  }

  private extractStrikeInfo(notice: any) {
    const text = (notice.title + ' ' + (notice.content || '')).toLowerCase();
    const originalText = notice.title + ' ' + (notice.content || '');

    // í‚¤ì›Œë“œ ê¸°ë°˜ íŒŒì—… ê°ì§€
    const strikeKeywords = ['íŒŒì—…', 'ìš´í–‰ì¤‘ë‹¨', 'ë…¸ì‚¬í˜‘ìƒ ê²°ë ¬'];
    const isStrike = strikeKeywords.some(keyword => text.includes(keyword));

    if (!isStrike) {
      return { isStrike: false };
    }

    // 1. ì§€ì—­ ì¶”ì¶œ (ì„œìš¸/ê²½ê¸°)
    const regions: string[] = [];
    if (text.includes('ì„œìš¸')) regions.push('seoul');
    if (text.includes('ê²½ê¸°')) regions.push('gyeonggi');
    if (text.includes('ì¸ì²œ')) regions.push('incheon');

    // 2. ë²„ìŠ¤ íƒ€ì… ì¶”ì¶œ (ì‹œë‚´ë²„ìŠ¤/ì‹œì™¸ë²„ìŠ¤)
    const busTypes: string[] = [];
    if (text.includes('ì‹œë‚´ë²„ìŠ¤') || text.includes('ì‹œë‚´ ë²„ìŠ¤')) busTypes.push('city');
    if (text.includes('ì‹œì™¸ë²„ìŠ¤') || text.includes('ì‹œì™¸ ë²„ìŠ¤') || text.includes('ê´‘ì—­')) busTypes.push('intercity');

    // ê¸°ë³¸ê°’: ëª…ì‹œ ì•ˆ ë˜ë©´ ì‹œë‚´ë²„ìŠ¤ë¡œ ê°„ì£¼
    if (busTypes.length === 0) busTypes.push('city');

    // 3. ë‚ ì§œ ì¶”ì¶œ
    const datePattern = /(\d{1,2})ì›”\s*(\d{1,2})ì¼/g;
    const dates = [...originalText.matchAll(datePattern)];
    const strikeDate = dates[0] ? this.parseExtractedDate(dates[0]) : null;

    // 4. ì‹œê°„ ì¶”ì¶œ (ì˜¤ì „/ì˜¤í›„ í¬í•¨)
    const timePattern = /(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2})\s*ì‹œ/g;
    const times = [...originalText.matchAll(timePattern)];
    let strikeTime = null;

    if (times.length > 0) {
      const timeMatch = times[0];
      let hour = parseInt(timeMatch[2]);
      if (timeMatch[1] === 'ì˜¤í›„' && hour < 12) hour += 12;
      if (timeMatch[1] === 'ì˜¤ì „' && hour === 12) hour = 0;
      strikeTime = `${hour.toString().padStart(2, '0')}:00:00`;
    }

    // 5. ìƒíƒœ íŒë‹¨ (ì˜ˆê³ /íŒŒì—…ì¤‘/íŒŒì—…ì¤‘ë‹¨)
    let status = 'scheduled'; // ê¸°ë³¸ê°’: ì˜ˆê³ 

    if (text.includes('íŒŒì—…ì¤‘ë‹¨') || text.includes('íŒŒì—… ì¤‘ë‹¨') || text.includes('íŒŒì—… ì² íšŒ')) {
      status = 'cancelled';
    } else if (text.includes('íŒŒì—… ëŒì…') || text.includes('íŒŒì—… ì‹œì‘') || text.includes('íŒŒì—… ì¤‘')) {
      status = 'ongoing';
    } else if (text.includes('íŒŒì—… ì˜ˆì •') || text.includes('íŒŒì—… ê³„íš') || text.includes('íŒŒì—… ì˜ˆê³ ')) {
      status = 'scheduled';
    }

    return {
      isStrike: true,
      title: notice.title,
      regions,
      busTypes,
      strikeDate,
      strikeTime,
      status,
      sourceUrl: notice.link || notice.url
    };
  }

  private async findSimilarStrikeEvent(strikeInfo: any): Promise<any> {
    // ê°™ì€ ë‚ ì§œ, ê°™ì€ ì§€ì—­ì˜ íŒŒì—… ì´ë²¤íŠ¸ ì°¾ê¸°
    const result = await db.query(
      `SELECT * FROM strike_events
       WHERE strike_date = $1
       AND affected_regions && $2
       AND status != 'ended'
       ORDER BY detected_at DESC
       LIMIT 1`,
      [strikeInfo.strikeDate, strikeInfo.regions]
    );

    return result.rows[0] || null;
  }

  private async updateStrikeEventIfChanged(
    existingStrike: any,
    newInfo: any,
    rawNoticeId: string,
    source: string
  ) {
    let hasChanges = false;
    const changes: string[] = [];

    // ìƒíƒœ ë³€ê²½ ì²´í¬
    if (existingStrike.status !== newInfo.status) {
      hasChanges = true;
      changes.push(`status: ${existingStrike.status} â†’ ${newInfo.status}`);
    }

    // ì‹œê°„ ë³€ê²½ ì²´í¬
    if (existingStrike.strike_time !== newInfo.strikeTime) {
      hasChanges = true;
      changes.push(`time: ${existingStrike.strike_time} â†’ ${newInfo.strikeTime}`);
    }

    // ì§€ì—­ ë³€ê²½ ì²´í¬
    const oldRegions = existingStrike.affected_regions || [];
    const newRegions = newInfo.regions || [];
    if (JSON.stringify(oldRegions.sort()) !== JSON.stringify(newRegions.sort())) {
      hasChanges = true;
      changes.push(`regions: ${oldRegions.join(',')} â†’ ${newRegions.join(',')}`);
    }

    // ë²„ìŠ¤ íƒ€ì… ë³€ê²½ ì²´í¬
    const oldBusTypes = existingStrike.bus_types || [];
    const newBusTypes = newInfo.busTypes || [];
    if (JSON.stringify(oldBusTypes.sort()) !== JSON.stringify(newBusTypes.sort())) {
      hasChanges = true;
      changes.push(`busTypes: ${oldBusTypes.join(',')} â†’ ${newBusTypes.join(',')}`);
    }

    if (hasChanges) {
      // ì—…ë°ì´íŠ¸
      await db.query(
        `UPDATE strike_events
         SET status = $1,
             strike_time = $2,
             affected_regions = $3,
             bus_types = $4,
             raw_notice_id = $5,
             source = $6,
             last_updated_at = NOW()
         WHERE id = $7`,
        [
          newInfo.status,
          newInfo.strikeTime,
          newInfo.regions,
          newInfo.busTypes,
          rawNoticeId,
          source,
          existingStrike.id
        ]
      );

      logger.info(`Strike event updated: ${existingStrike.id}`);
      logger.info(`Changes: ${changes.join(', ')}`);

      // ğŸ”” ì—…ë°ì´íŠ¸ ì•Œë¦¼ ë°œì†¡
      await this.sendNotification(existingStrike.id, 'update', changes);
    }
  }

  private async createStrikeEvent(
    rawNoticeId: string,
    strikeInfo: any,
    source: string,
    sendNotification: boolean = false
  ) {
    const id = generateId('strike');

    const startDatetime = strikeInfo.strikeDate && strikeInfo.strikeTime
      ? new Date(`${strikeInfo.strikeDate.toISOString().split('T')[0]} ${strikeInfo.strikeTime}`)
      : strikeInfo.strikeDate;

    await db.query(
      `INSERT INTO strike_events
       (id, raw_notice_id, title, status, strike_date, strike_time,
        start_datetime, affected_regions, bus_types, source, source_url)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)`,
      [
        id,
        rawNoticeId,
        strikeInfo.title,
        strikeInfo.status,
        strikeInfo.strikeDate,
        strikeInfo.strikeTime,
        startDatetime,
        strikeInfo.regions,
        strikeInfo.busTypes,
        source,
        strikeInfo.sourceUrl
      ]
    );

    logger.info(`Strike event created: ${id} (status: ${strikeInfo.status})`);

    // ğŸ”” ì‹ ê·œ íŒŒì—… ì•Œë¦¼ ë°œì†¡
    if (sendNotification) {
      await this.sendNotification(id, 'new');
    }
  }

  private async sendNotification(strikeEventId: string, type: 'new' | 'update', changes?: string[]) {
    // íŒŒì—… ì´ë²¤íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    const result = await db.query(
      'SELECT * FROM strike_events WHERE id = $1',
      [strikeEventId]
    );

    if (result.rows.length === 0) return;

    const strike = result.rows[0];

    // FCM ì•Œë¦¼ ë©”ì‹œì§€ êµ¬ì„±
    const message = {
      notification: {
        title: type === 'new'
          ? `[${this.formatRegions(strike.affected_regions)}] ë²„ìŠ¤ íŒŒì—… ${this.formatStatus(strike.status)}`
          : `[${this.formatRegions(strike.affected_regions)}] íŒŒì—… ì •ë³´ ì—…ë°ì´íŠ¸`,
        body: type === 'new'
          ? `${strike.strike_date} ${strike.strike_time || ''} - ${this.formatBusTypes(strike.bus_types)}`
          : `ë³€ê²½ì‚¬í•­: ${changes?.join(', ')}`
      },
      data: {
        type: type === 'new' ? 'strike_created' : 'strike_updated',
        strikeId: strike.id,
        status: strike.status,
        regions: JSON.stringify(strike.affected_regions),
        busTypes: JSON.stringify(strike.bus_types),
        strikeDate: strike.strike_date?.toString() || '',
        strikeTime: strike.strike_time || '',
        changes: type === 'update' ? JSON.stringify(changes) : ''
      }
    };

    // TODO: FCM ë°œì†¡ êµ¬í˜„
    // await admin.messaging().send(message);

    logger.info(`Notification queued for strike ${strikeEventId}: ${type}`);
  }

  private formatRegions(regions: string[]): string {
    const map: any = { seoul: 'ì„œìš¸', gyeonggi: 'ê²½ê¸°', incheon: 'ì¸ì²œ' };
    return regions.map(r => map[r] || r).join('/');
  }

  private formatBusTypes(busTypes: string[]): string {
    const map: any = { city: 'ì‹œë‚´ë²„ìŠ¤', intercity: 'ì‹œì™¸ë²„ìŠ¤' };
    return busTypes.map(t => map[t] || t).join('/');
  }

  private formatStatus(status: string): string {
    const map: any = {
      scheduled: 'ì˜ˆê³ ',
      ongoing: 'íŒŒì—…ì¤‘',
      cancelled: 'íŒŒì—…ì¤‘ë‹¨',
      ended: 'ì¢…ë£Œ'
    };
    return map[status] || status;
  }

  private parseDate(dateStr: string): Date | undefined {
    // YYYY.MM.DD í˜•ì‹ íŒŒì‹±
    const match = dateStr.match(/(\d{4})\.(\d{1,2})\.(\d{1,2})/);
    if (match) {
      return new Date(parseInt(match[1]), parseInt(match[2]) - 1, parseInt(match[3]));
    }
    return undefined;
  }

  private parseExtractedDate(match: RegExpMatchArray): Date {
    const month = parseInt(match[1]);
    const day = parseInt(match[2]);
    const year = new Date().getFullYear();
    return new Date(year, month - 1, day);
  }
}

export const strikeService = new StrikeService();
```

### Step 6: ìŠ¤ì¼€ì¤„ëŸ¬

```typescript
// src/scheduler.ts
import cron from 'node-cron';
import { TOPISCrawler } from './crawlers/topis';
import { GBISCrawler } from './crawlers/gbis';
import { strikeService } from './services/strike';
import { db } from './models/database';
import { redis } from './utils/redis';
import { generateId } from './utils/hash';
import { logger } from './utils/logger';

class CrawlerScheduler {
  private crawlers: Map<string, any> = new Map();

  async initialize() {
    // í¬ë¡¤ëŸ¬ ë“±ë¡
    this.crawlers.set('topis', new TOPISCrawler());
    this.crawlers.set('gbis', new GBISCrawler());

    // í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    for (const [id, crawler] of this.crawlers) {
      await crawler.initialize();
    }

    logger.info('Crawler scheduler initialized');
  }

  start() {
    // Tier 1: 30ë¶„ë§ˆë‹¤
    cron.schedule('*/30 * * * *', async () => {
      logger.info('Starting Tier 1 crawling...');
      await this.crawlTier1();
    });

    // ì¦‰ì‹œ ì‹¤í–‰ (ì²« ì‹œì‘)
    this.crawlTier1();

    logger.info('Crawler scheduler started');
  }

  private async crawlTier1() {
    const tier1Crawlers = ['topis', 'gbis'];

    for (const crawlerId of tier1Crawlers) {
      await this.runCrawler(crawlerId);
    }
  }

  private async runCrawler(crawlerId: string) {
    const crawler = this.crawlers.get(crawlerId);
    if (!crawler) {
      logger.error(`Crawler not found: ${crawlerId}`);
      return;
    }

    // ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (ë½ íšë“)
    const lockKey = `crawl:lock:${crawlerId}`;
    const locked = await redis.setNX(lockKey, '1', 300); // 5ë¶„ íƒ€ì„ì•„ì›ƒ

    if (!locked) {
      logger.warn(`Crawler already running: ${crawlerId}`);
      return;
    }

    const crawlLogId = generateId('log');
    const startTime = Date.now();

    try {
      // í¬ë¡¤ë§ ì‹œì‘
      logger.info(`Starting crawler: ${crawlerId}`);
      const result = await crawler.crawl();

      // ë¡œê·¸ ì €ì¥
      await db.query(
        `INSERT INTO crawl_logs
         (id, source_id, status, items_found, items_processed, duration_ms)
         VALUES ($1, $2, $3, $4, $5, $6)`,
        [
          crawlLogId,
          crawlerId,
          result.error ? 'failed' : 'success',
          result.items.length,
          0, // ì•„ì§ ì²˜ë¦¬ ì•ˆ ë¨
          result.duration
        ]
      );

      // ìˆ˜ì§‘ëœ ê³µì§€ì‚¬í•­ ì²˜ë¦¬
      let processedCount = 0;
      for (const item of result.items) {
        try {
          await strikeService.processNotice(crawlLogId, crawlerId, item);
          processedCount++;
        } catch (error) {
          logger.error(`Failed to process notice:`, error);
        }
      }

      // ì²˜ë¦¬ ì™„ë£Œ ì—…ë°ì´íŠ¸
      await db.query(
        'UPDATE crawl_logs SET items_processed = $1 WHERE id = $2',
        [processedCount, crawlLogId]
      );

      // ì†ŒìŠ¤ ì—…ë°ì´íŠ¸
      await db.query(
        'UPDATE crawl_sources SET last_crawled_at = NOW() WHERE id = $1',
        [crawlerId]
      );

      logger.info(`Crawler completed: ${crawlerId} (${result.items.length} items, ${processedCount} processed)`);

    } catch (error: any) {
      logger.error(`Crawler error: ${crawlerId}`, error);

      // ì—ëŸ¬ ë¡œê·¸ ì €ì¥
      await db.query(
        `INSERT INTO crawl_logs
         (id, source_id, status, items_found, duration_ms, error_message)
         VALUES ($1, $2, $3, $4, $5, $6)`,
        [
          crawlLogId,
          crawlerId,
          'failed',
          0,
          Date.now() - startTime,
          error.message
        ]
      );
    } finally {
      // ë½ í•´ì œ
      await redis.del(lockKey);
    }
  }

  async cleanup() {
    for (const [id, crawler] of this.crawlers) {
      await crawler.cleanup();
    }
    logger.info('Crawler scheduler stopped');
  }
}

export const scheduler = new CrawlerScheduler();
```

### Step 7: ë©”ì¸ ì§„ì…ì 

```typescript
// src/index.ts
import dotenv from 'dotenv';
import { scheduler } from './scheduler';
import { redis } from './utils/redis';
import { logger } from './utils/logger';

dotenv.config();

async function main() {
  try {
    logger.info('Starting crawler service...');

    // Redis ì—°ê²°
    await redis.connect();

    // ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ë° ì‹œì‘
    await scheduler.initialize();
    scheduler.start();

    logger.info('Crawler service started successfully');

    // Graceful shutdown
    process.on('SIGINT', async () => {
      logger.info('Shutting down...');
      await scheduler.cleanup();
      process.exit(0);
    });

  } catch (error) {
    logger.error('Failed to start crawler service:', error);
    process.exit(1);
  }
}

main();
```

### Step 8: package.json ìŠ¤í¬ë¦½íŠ¸

```json
{
  "scripts": {
    "dev": "nodemon --exec ts-node src/index.ts",
    "build": "tsc",
    "start": "node dist/index.js",
    "test": "jest",
    "init-db": "docker exec -i postgres psql -U busstrike -d busstrike < scripts/init-db.sql",
    "seed": "docker exec -i postgres psql -U busstrike -d busstrike < scripts/seed-sources.sql"
  }
}
```

---

## 4. í…ŒìŠ¤íŠ¸

### 4.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```typescript
// tests/services/strike.test.ts
import { StrikeService } from '../../src/services/strike';

describe('StrikeService', () => {
  const service = new StrikeService();

  test('should detect strike from title', () => {
    const notice = {
      title: 'ì„œìš¸ ì‹œë‚´ë²„ìŠ¤ íŒŒì—… ì˜ˆì • ì•ˆë‚´',
      date: '2026.01.15',
      content: ''
    };

    const info = (service as any).extractStrikeInfo(notice);
    expect(info.isStrike).toBe(true);
    expect(info.regions).toContain('seoul');
  });

  test('should extract dates', () => {
    const notice = {
      title: 'íŒŒì—… ì•ˆë‚´',
      content: '1ì›” 15ì¼ë¶€í„° 1ì›” 17ì¼ê¹Œì§€ íŒŒì—…',
      date: ''
    };

    const info = (service as any).extractStrikeInfo(notice);
    expect(info.startDate).toBeDefined();
    expect(info.endDate).toBeDefined();
  });
});
```

### 4.2 í†µí•© í…ŒìŠ¤íŠ¸

```bash
# í¬ë¡¤ëŸ¬ ìˆ˜ë™ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
npm run dev

# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ë¡œê·¸ í™•ì¸
tail -f logs/crawler.log

# ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
docker exec -it postgres psql -U busstrike -d busstrike
SELECT * FROM crawl_logs ORDER BY crawled_at DESC LIMIT 5;
SELECT * FROM raw_notices ORDER BY crawled_at DESC LIMIT 5;
SELECT * FROM strike_events;
```

---

## 5. ë°°í¬

### 5.1 Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM node:18-alpine

WORKDIR /app

# Install Playwright dependencies
RUN apk add --no-cache \
    chromium \
    nss \
    freetype \
    harfbuzz \
    ca-certificates \
    ttf-freefont

# Set Playwright to use local chromium
ENV PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD=1
ENV PLAYWRIGHT_CHROMIUM_EXECUTABLE_PATH=/usr/bin/chromium-browser

# Install dependencies
COPY package*.json ./
RUN npm ci --production

# Copy source
COPY dist ./dist

CMD ["node", "dist/index.js"]
```

### 5.2 docker-compose.yml

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: busstrike
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: busstrike
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  crawler:
    build: .
    depends_on:
      - postgres
      - redis
    environment:
      DATABASE_URL: postgresql://busstrike:${DB_PASSWORD}@postgres:5432/busstrike
      REDIS_URL: redis://redis:6379
    restart: unless-stopped

volumes:
  postgres_data:
```

---

## 6. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: Playwright íƒ€ì„ì•„ì›ƒ

```typescript
// í•´ê²°: íƒ€ì„ì•„ì›ƒ ëŠ˜ë¦¬ê¸°
await page.goto(url, { waitUntil: 'networkidle', timeout: 60000 });

// ë˜ëŠ” waitUntil ë³€ê²½
await page.goto(url, { waitUntil: 'domcontentloaded' });
```

### ë¬¸ì œ 2: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜

```typescript
// í•´ê²°: í˜ì´ì§€ ëª…ì‹œì  ì¢…ë£Œ
const page = await browser.newPage();
try {
  // í¬ë¡¤ë§ ë¡œì§
} finally {
  await page.close();
}
```

### ë¬¸ì œ 3: ë™ì‹œ ì‹¤í–‰ ì œì–´

```typescript
// Redis ë½ ì‚¬ìš©
const lockKey = `crawl:lock:${sourceId}`;
const locked = await redis.setNX(lockKey, '1', 300);
if (!locked) return;
```

---

## ë‹¤ìŒ ë‹¨ê³„

1. **Admin API êµ¬í˜„**: Expressë¡œ ê´€ë¦¬ API êµ¬ì¶•
2. **ì•Œë¦¼ ì—°ë™**: FCM í‘¸ì‹œ ì•Œë¦¼ ë°œì†¡
3. **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana
4. **ML ëª¨ë¸**: íŒŒì—… ê°ì§€ ì •í™•ë„ í–¥ìƒ
5. **ë¶„ì‚° í¬ë¡¤ë§**: ë‹¤ì¤‘ ì„œë²„ í™˜ê²½ ì§€ì›